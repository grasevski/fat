# -*- coding: utf-8 -*-
"""Copy of NASnet_AGING_EVO.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1POgcj3cdqGDvbfpLEUxr4byf0TJjLMQU
"""
import numpy as np
import pandas as pd
import matplotlib as plt
from sklearn.metrics import multilabel_confusion_matrix
from sklearn.ensemble import RandomForestClassifier
from sklearn import svm
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import OneHotEncoder
import time
import imblearn
from imblearn.over_sampling import SMOTE
import keras
import collections
from collections import Counter
import operator
import logging
import os
import pathlib
import re
import string
import sys
import pickle
import math
import numpy
import random
import itertools
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.datasets.mnist import load_data
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import MaxPool2D
from tensorflow.keras.layers import AveragePooling2D
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import Lambda
from keras.layers.normalization import BatchNormalization
from keras.utils import to_categorical
from sklearn import svm
from sklearn.preprocessing import MinMaxScaler
import talib as ta



"""# Data loading and preprocessing

## Indicators
"""

def PSAR (AFrate):
    direction = [None,None,None]
    psar =[None,None,None]
    EP = [None,None,None]
    pri_EP_count = 1
    AF = min(pri_EP_count*AFrate,0.2)
    if high[4]> high[3]:
        psar.append(high[3])
        EP.append(high[3])
        direction.append('F')
    else:
        psar.append(low[3])
        EP.append(low[3])
        direction.append('F')
    for i in range(4,len(low)):
        if low[i]<psar[i-1] and direction[i-1] == 'R':
            direction.append('F')
            AF = 0.02
            npsar = EP[i-1] - AF*(EP[i-1] - psar[i-1])
            psar.append(round(npsar,2))
            pri_EP_count = 1
            if low[i]< EP[i-1]:
                EP.append(low[i])
            else:
                EP.append(EP[i-1])
        elif high[i]<=psar[i-1] and direction[i-1] == 'F':
            if low[i]< EP[i-1]:
                pri_EP_count = pri_EP_count + 1
                EP.append(low[i])
            else:
                EP.append(EP[i-1])
            direction.append('F')
            AF = min(pri_EP_count*AFrate,0.2)
            npsar = psar[i-1] - AF*(psar[i-1] - min(low[i-4],low[i-3],low[i-2],low[i-1],low[i]))
            psar.append(round(npsar,2))
        elif low[i]>=psar[i-1] and direction[i-1] == 'R':
            if high[i]> EP[i-1]:
                pri_EP_count = pri_EP_count + 1
                EP.append(high[i])
            else:
                EP.append(EP[i-1])
            direction.append('R')
            AF = min(pri_EP_count*AFrate,0.2)
            npsar = psar[i-1] + AF*(max(high[i-1],high[i]) - psar[i-1])
            psar.append(round(npsar,2))
            
        elif high[i]> psar[i-1] and direction[i-1] == 'F':
            direction.append('R')
            AF = 0.02
            npsar = low[i-1]
            psar.append(round(npsar,2))
            pri_EP_count = 1
            if high[i]> EP[i-1]:
                EP.append(high[i])
            else:
                EP.append(EP[i-1])
        else:
            print('DIRECTION_ERROR')
    return psar


def DMA(series,weight):
    a = series[0]
    Y = [a]
    for i in range(1,len(series)):
        b = weight[i]*series[i] + (1-weight[i])*Y[i-1]
        Y.append(b)
    return Y

def WRF(n = 6):
    v1 = []
    for i in range(len(closing_price)):
        h_l = [h for h in list(high[max(0,i+1-n):i+1])]
        h_n = np.float(max(h_l))
        l_l = [l for l in list(low[max(0,i+1-n):i+1])]
        l_n = np.float(min(l_l))
        a = np.float(h_n - l_n)
        b = np.float(h_n - closing_price[i])
        if a != 0:
            c = np.float(b/a)
            d = c*100
        else:
            d = 0
        v1.append(d)
    return pd.Series(v1)

def CYHT(V_period = 34, E_period = 13):
    var2 =low.rolling(V_period).min()
    var3 =high.rolling(V_period).max()
    var1 = (2*closing_price+high+low+open_l)/5
    SK =ta.EMA((((var1-var2)/(var3-var2))*100),E_period)
    SD = ta.EMA(SK,3)
    return SK, SD

def CJDX ():
    Var1 = (2*closing_price+high+low)/4
    Var2 = (4*Var1+3*Var1.shift(1)+2*Var1.shift(2)+Var1.shift(3))/10
    Var3 = (4*Var2+3*Var2.shift(1)+2*Var2.shift(2)+Var2.shift(3))/10
    Var4 = (4*Var3+3*Var3.shift(1)+2*Var3.shift(2)+Var3.shift(3))/10
    J = (Var4 - Var4.shift(1))*100/Var4.shift(1)
    D = J.rolling(3).mean()
    return J, D

def scaling(data,seq_len):
    scaler = MinMaxScaler()
    scaled_data =[]
    scaler.fit(data.to_numpy().reshape(-1, 1))
    for i in range(len(data)):
        scaled_data.append(scaler.transform(data[i].reshape(-1, 1)))
    final = np.array(scaled_data).flatten()
    return final

def label(data):
  label_ = []
  for i in range(len(data)-3):
    if (closing_price[i+3]- closing_price[i])/closing_price[i]>0.0012:
      label_.append(2)
    elif (closing_price[i+3]- closing_price[i])/closing_price[i]<-0.0012:
      label_.append(0)
    else:
      label_.append(1)
  label_.append(1)
  label_.append(1)
  label_.append(1)
  return label_

original_data = pd.read_csv('EUR_JPY_M30.csv')

del original_data['datetime']

## Preprocessing data

closing_price = original_data['close']
open_l = original_data['open']
high = original_data['high']
low = original_data['low']
rsi_6 = ta.RSI(closing_price, timeperiod=6)
rsi_12 = ta.RSI(closing_price, timeperiod=12)
rsi_24 = ta.RSI(closing_price, timeperiod=24)
CJDX_J,CJDX_D = CJDX()
rsi_dif = rsi_6 - rsi_24
MA_5 = ta.SMA(closing_price, timeperiod=5) / original_data['close']
MA_10 = ta.SMA(closing_price, timeperiod=10) / original_data['close']
MA_55 = ta.SMA(closing_price, timeperiod=55) / original_data['close']
MA_144 = ta.SMA(closing_price, timeperiod=144) / original_data['close']
MA_20 = ta.SMA(closing_price, timeperiod=20) / original_data['close']
MA_dif = MA_5 - MA_10
SK_L, SD_L = CYHT()
ATR = ta.ATR(high, low, closing_price, timeperiod=14)
EMA_12 = ta.EMA(closing_price, timeperiod=12) / original_data['close']
EMA_26 = ta.EMA(closing_price, timeperiod=26) / original_data['close']
dif,dea,macd = ta.MACD(closing_price)
upperband, middleband, lowerband = ta.BBANDS(closing_price, timeperiod=20, nbdevup=2, nbdevdn=2, matype=0)
lb = lowerband / closing_price
WR_L = WRF()
WR = WR_L
ma5 = MA_5
ma10 = MA_10
ma55 = MA_55
ma144 = MA_144
ma20 = MA_20
PASR_L = pd.Series(PSAR(0.02))
PSAR_S = ((PASR_L - closing_price)/closing_price)*100
l = label(original_data)
print(l.count(0))
print(l.count(1))
print(l.count(2))

original_data['rsi_6'] = rsi_6
original_data['rsi_12'] = rsi_12
original_data['rsi_12_dif'] = original_data['rsi_12'].diff()
original_data['close_dif1'] = original_data['close'].pct_change(periods = 1)
original_data['close_dif2'] = original_data['close'].pct_change(periods = 2)
original_data['close_dif3'] = original_data['close'].pct_change(periods = 3)
original_data['rsi_24'] = rsi_24
original_data['rsi_dif'] = (rsi_6 - rsi_24)
original_data['J'] = CJDX_J
original_data['D'] = CJDX_D
original_data['JD'] = (CJDX_J - CJDX_D)
original_data['MA_5'] = MA_5
original_data['MA_10'] = MA_10
original_data['MA_20'] = ta.SMA(closing_price, timeperiod=20) / original_data['close']
original_data['MA_55'] = ta.SMA(closing_price, timeperiod=55) / original_data['close']
original_data['MA_dif'] = MA_dif
original_data['SD'] = SD_L
original_data['SK'] = SK_L
original_data['CYHT'] = SK_L - SD_L
original_data['EMA_12'] = EMA_12
original_data['EMA_26'] = EMA_26
original_data['macd'] = macd
original_data['BBupperband'] = (upperband / closing_price)
original_data['BBlowerband'] = (lowerband / closing_price)
original_data['bandwidth'] = (upperband / closing_price) - (lowerband / closing_price)
original_data['WR'] = WR_L
original_data['PSAR'] = PSAR_S
original_data['label'] = l

original_data = original_data[146:-4]
print(original_data)
##original_data = pd.read_csv('source_data.csv')
def data_sequence(data, seq_len):    #### Data needs to be an array
    X = []
    y = []
    for i in range(seq_len,len(data)):
        truncated_data = data[i-seq_len:i]
        truncated_data = truncated_data.reset_index()
        for nam in truncated_data:
          if nam != 'label' and nam != 'index':
            truncated_data[str(nam)] = scaling(truncated_data[str(nam)],seq_len)
          else:
            pass
        if truncated_data['label'][len(truncated_data)-1] == 0:
            temp = [1,0,0]
            y.append(temp)
        elif truncated_data['label'][len(truncated_data)-1] == 1:
            temp = [0,1,0]
            y.append(temp)
        else:
            temp = [0,0,1]
            y.append(temp)
        del truncated_data['label']
        del truncated_data['index']
        truncated_data['CLS'] = truncated_data.mean(axis=1)
        X.append(truncated_data)

    return np.array(X), np.array(y)


seq_len = 35

### Data prep
X,y = data_sequence(original_data, seq_len)

print(X)
print(y)
print(X.shape)
print(y.shape)
with open('D:/Uni work/DATA5709/preprocessed_Xdata_withCLS.npy', 'wb') as f:
    np.save(f,X)
with open('D:/Uni work/DATA5709/preprocessed_ydata_withCLS.npy', 'wb') as g:
    np.save(g,y)
print('Data Saved')
##with open('D:/Uni work/DATA5709/preprocessed_Xdata.npy', 'rb') as h:
##    X = np.load(h)
##with open('D:/Uni work/DATA5709/preprocessed_ydata.npy', 'rb') as j:
##    y = np.load(j)
##print('Data Loaded')
##print(X)
##print(y)
##print(X.shape)
##print(y.shape)
## Split data into train and test
split_point = int(len(X)*0.8) 
X_train = np.asarray(X[:split_point])
X_train = tf.cast(X_train, dtype=tf.float32)
X_test = np.asarray(X[split_point:])
X_test = tf.cast(X_test, dtype=tf.float32)

y_train = np.asarray(y[:split_point])  
y_train = tf.cast(y_train, dtype=tf.float32)  
y_test = np.asarray(y[split_point:])  
y_test = tf.cast(y_test, dtype=tf.float32)    


print("X_train shape:",X_train.shape)
print("X_test shape:",X_test.shape)


print("y_train shape:",y_train.shape)
print("y_test shape:",y_test.shape)

"""# Attention Layers"""

def get_angles(pos, i, d_model):
  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))
  return pos * angle_rates

def positional_encoding(position, d_model):
  angle_rads = get_angles(np.arange(position)[:, np.newaxis],
                          np.arange(d_model)[np.newaxis, :],
                          d_model)

  # apply sin to even indices in the array; 2i
  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])

  # apply cos to odd indices in the array; 2i+1
  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])

  pos_encoding = angle_rads[np.newaxis, ...]

  return tf.cast(pos_encoding, dtype=tf.float32)

def scaled_dot_product_attention(q, k, v):

  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)

  # scale matmul_qk
  dk = tf.cast(tf.shape(k)[-1], tf.float32)
  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)

  # softmax is normalized on the last axis (seq_len_k) so that the scores
  # add up to 1.
  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)

  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)

  return output, attention_weights

class MultiHeadAttention(tf.keras.layers.Layer):
  def __init__(self, d_model, num_heads):
    super(MultiHeadAttention, self).__init__()
    self.num_heads = num_heads
    self.d_model = d_model

    assert d_model % self.num_heads == 0

    self.depth = d_model // self.num_heads

    self.wq = tf.keras.layers.Dense(d_model)
    self.wk = tf.keras.layers.Dense(d_model)
    self.wv = tf.keras.layers.Dense(d_model)

    self.dense = tf.keras.layers.Dense(d_model)

  def split_heads(self, x, batch_size):
    """Split the last dimension into (num_heads, depth).
    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)
    """
    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))
    return tf.transpose(x, perm=[0, 2, 1, 3])

  def call(self, v, k, q):
    
    batch_size = tf.shape(q)[0]

    q = self.wq(q)  # (batch_size, seq_len, d_model)
    k = self.wk(k)  # (batch_size, seq_len, d_model)
    v = self.wv(v)  # (batch_size, seq_len, d_model)

    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)
    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)
    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)

    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)
    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)
    scaled_attention, attention_weights = scaled_dot_product_attention(
        q, k, v)

    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)

    concat_attention = tf.reshape(scaled_attention,
                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)

    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)

    return output, attention_weights

def point_wise_feed_forward_network(d_model, dff):
  return tf.keras.Sequential([
      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)
      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)
  ])

"""# AGEING EVOLUTION"""

def randarchitecture (max_depth):
  outputs = []
  for i in range(max_depth):
    outputs.append('x_{}'.format(i))
  architecture = {}
  generated_outputs=['input']
  used_outputs = []
  for b in range (max_depth):
    if b == 0:
      input_1 = random.choice(generated_outputs)
      input_2 = random.choice(generated_outputs)
      used_outputs.append(input_1)
      used_outputs.append(input_2)
      generated_outputs.append(outputs[b])
      architecture[b] = (input_1,input_2 , 'mha',generated_outputs[-1])
    else:
      a = random.randint(1,2)
      if a == 1:
        input_1 = random.choice(generated_outputs)
        input_2 = random.choice(generated_outputs)
        used_outputs.append(input_1)
        used_outputs.append(input_2)
        generated_outputs.append(outputs[b])
        architecture[b] = (input_1,input_2 , 'mha',generated_outputs[-1])
      else:
        input = random.choice(generated_outputs)
        used_outputs.append(input)
        generated_outputs.append(outputs[b])
        architecture[b] = (input,None, 'ff',generated_outputs[-1])
  used_outputs = set(used_outputs)
  g = set(generated_outputs)
  unused_outputs = g.difference(used_outputs)
  unused_outputs = list(unused_outputs)
  used_outputs = list(used_outputs)
  d= max_depth
  if unused_outputs:
    while len(unused_outputs) > 1:
      used_outputs.append(unused_outputs[0])
      used_outputs.append(unused_outputs[1])
      generated_outputs.append('x_{}'.format(d))
      architecture[d] = (unused_outputs[0],unused_outputs[1], 'mha',generated_outputs[-1])
      d = d+1
      used_outputs = set(used_outputs)
      g = set(generated_outputs)
      unused_outputs = g.difference(used_outputs)
      unused_outputs = list(unused_outputs)
      used_outputs = list(used_outputs)
  return architecture

def mutate_rewire (architecture):
  print('rewire')
  rewire_layer = random.choice(list(architecture.keys())[:-1])
  while architecture[rewire_layer][2] != 'mha':
    rewire_layer = random.choice(list(architecture.keys())[:-1])
  potential_inputs= ['input']
  for i in range(rewire_layer):
    potential_inputs.append('x_{}'.format(i))
  layer = (architecture[rewire_layer][0],random.choice(potential_inputs),architecture[rewire_layer][2],architecture[rewire_layer][3])
  architecture[rewire_layer] = layer
  used_input = []
  used_output = []
  d = 0
  for a in architecture.items():
    if a[1][2] == 'mha':
      used_input.append(a[1][0])
      used_input.append(a[1][1])
    elif a[1][2] == 'ff':
      used_input.append(a[1][0])
    else:
      pass
    d = d+1
    used_output.append(a[1][3])
  used_input = set(used_input)
  used_output = set(used_output)
  unused_outputs = used_output.difference(used_input)
  unused_outputs = list(unused_outputs)
  used_output = list(used_output)
  used_input = list(used_input)
  if unused_outputs:
    while len(unused_outputs) > 1:
      used_input.append(unused_outputs[0])
      used_input.append(unused_outputs[1])
      architecture[d] = (unused_outputs[0],unused_outputs[1], 'mha','x_{}'.format(d))
      used_output.append('x_{}'.format(d))
      used_input = set(used_input)
      g = set(used_output)
      unused_outputs = g.difference(used_input)
      unused_outputs = list(unused_outputs)
      used_input = list(used_input)
      d = d+1
  return architecture

def mutate_add_sub (architecture):
  a = random.randint(1,2)
  layers = list(architecture.keys())
  potential_inputs= ['input']
  for b in architecture.items():
    potential_inputs.append(b[1][3])
  if a == 1 and len(architecture)<=100: 
    print('add')
    added_layer = len(layers)
    architecture[added_layer] = (random.choice(potential_inputs),random.choice(potential_inputs), 'mha','x_{}'.format(added_layer))
  else:
    print('sub')
    deleted = random.choice(layers)
    if deleted == layers[-1]:
      del architecture[deleted]
    else:     
      potential_inputs= ['input']
      del architecture[deleted]
      for i in range(deleted):
        potential_inputs.append('x_{}'.format(i))
      for x in range(deleted+1,len(architecture)+1):
        if architecture[x][2] == 'mha':
          if architecture[x][0] not in potential_inputs:
            if architecture[x][1] not in potential_inputs:
              input1 = random.choice(potential_inputs)
              input2 = random.choice(potential_inputs)
              del architecture[x]
              architecture[(x-1)] = (input1,input2,'mha','x_{}'.format((x-1)))
              potential_inputs.append('x_{}'.format((x-1)))
            else:
              input1 = random.choice(potential_inputs)
              input2 = architecture[x][1]
              del architecture[x]
              architecture[(x-1)] = (input1,input2,'mha','x_{}'.format((x-1)))
              potential_inputs.append('x_{}'.format((x-1)))
          else:
            if architecture[x][1] not in potential_inputs:
              input1 = architecture[x][0]
              input2 = random.choice(potential_inputs)
              del architecture[x]
              architecture[(x-1)] = (input1,input2,'mha','x_{}'.format((x-1)))
              potential_inputs.append('x_{}'.format((x-1)))
            else:
              input1 = architecture[x][0]
              input2 = architecture[x][1]
              del architecture[x]
              architecture[(x-1)] = (input1,input2,'mha','x_{}'.format((x-1)))
              potential_inputs.append('x_{}'.format((x-1)))
        else:
          if architecture[x][0] not in potential_inputs:
            input1 = random.choice(potential_inputs)
            del architecture[x]
            architecture[(x-1)] = (input1,None,'ff','x_{}'.format((x-1)))
            potential_inputs.append('x_{}'.format((x-1)))
          else:
            input1 = architecture[x][0]
            del architecture[x]
            architecture[(x-1)] = (input1,None,'ff','x_{}'.format((x-1)))
            potential_inputs.append('x_{}'.format((x-1)))
  used_input = []
  used_output = []
  d = 0
  for a in architecture.items():
    if a[1][2] == 'mha':
      used_input.append(a[1][0])
      used_input.append(a[1][1])
    elif a[1][2] == 'ff':
      used_input.append(a[1][0])
    else:
      pass
    d = d+1
    used_output.append(a[1][3])
  used_input = set(used_input)
  used_output = set(used_output)
  unused_outputs = used_output.difference(used_input)
  unused_outputs = list(unused_outputs)
  used_output = list(used_output)
  used_input = list(used_input)
  if unused_outputs:
    while len(unused_outputs) > 1:
      used_input.append(unused_outputs[0])
      used_input.append(unused_outputs[1])
      architecture[d] = (unused_outputs[0],unused_outputs[1], 'mha','x_{}'.format(d))
      used_output.append('x_{}'.format(d))
      used_input = set(used_input)
      g = set(used_output)
      unused_outputs = g.difference(used_input)
      unused_outputs = list(unused_outputs)
      used_input = list(used_input)
      d = d+1
  return architecture

def hard_sub(architecture):
  print('resize')
  start = random.randint(50, 80)
  for x in range(start,len(architecture)):
    del architecture[x]
  used_input = []
  used_output = []
  d = 0
  for a in architecture.items():
    if a[1][2] == 'mha':
      used_input.append(a[1][0])
      used_input.append(a[1][1])
    elif a[1][2] == 'ff':
      used_input.append(a[1][0])
    else:
      pass
    d = d+1
    used_output.append(a[1][3])
  used_input = set(used_input)
  used_output = set(used_output)
  unused_outputs = used_output.difference(used_input)
  unused_outputs = list(unused_outputs)
  used_output = list(used_output)
  used_input = list(used_input)
  if unused_outputs:
    while len(unused_outputs) > 1:
      used_input.append(unused_outputs[0])
      used_input.append(unused_outputs[1])
      architecture[d] = (unused_outputs[0],unused_outputs[1], 'mha','x_{}'.format(d))
      used_output.append('x_{}'.format(d))
      used_input = set(used_input)
      g = set(used_output)
      unused_outputs = g.difference(used_input)
      unused_outputs = list(unused_outputs)
      used_input = list(used_input)
      d = d+1
  return architecture

def mutate(architecture):
  a =random.randint(1,3)
  if a == 1 and len(architecture)<=100:
    architecture = mutate_rewire(architecture)
  elif a != 1 and len(architecture)<=100: 
    architecture = mutate_add_sub (architecture)
  else:
    architecture = hard_sub(architecture)
  return architecture


"""## Building the architecture"""

class build (tf.keras.Model):
  def __init__(self, architecture, dff, d_model, maximum_position_encoding,num_heads,batch_size):
    super(build, self).__init__()
    self.pos_encoding = positional_encoding(maximum_position_encoding,
                                            d_model)
    self.mha = MultiHeadAttention(d_model, num_heads)
    self.ffn = point_wise_feed_forward_network(d_model, dff)
    self.layernorm = tf.keras.layers.LayerNormalization()
    self.flatten = tf.keras.layers.Flatten()
    self.dropout = tf.keras.layers.Dropout(0.1)
  def call(self, input, architecture, dff, d_model, num_heads,batch_size):
    i=1
    input += self.pos_encoding[:, :input.shape[1], :]
    initial_1,_ = self.mha(input,input,input)
    initial = self.ffn(initial_1)
    initial = self.layernorm(initial+initial_1)
    outputs = []
    for a in architecture.items():
      if a[1][2] == 'ff':
        if a[1][0] == 'input':
          t_x = point_wise_feed_forward_network(d_model, dff)(initial)
          globals()[a[1][-1]] = tf.keras.layers.LayerNormalization()(t_x+initial)
          outputs.append(eval(a[1][-1]))
        else:
          t_x = point_wise_feed_forward_network(d_model, dff)(eval(a[1][0]))
          globals()[a[1][-1]] = tf.keras.layers.LayerNormalization()(t_x+initial)
          outputs.append(eval(a[1][-1]))
      elif a[1][2] == 'mha':
        if a[1][0] == 'input':
          if a[1][1] == 'input':
            t_x,_ = MultiHeadAttention(d_model, num_heads)(initial,initial,initial)
            t_x = tf.keras.layers.LayerNormalization()(t_x+ initial)
            t_x1 = point_wise_feed_forward_network(d_model, dff)(t_x)
            globals()[a[1][-1]] = tf.keras.layers.LayerNormalization()(t_x+t_x1)
            outputs.append(eval(a[1][-1]))
            i = i+1
          else:
            t_x,_ = MultiHeadAttention(d_model, num_heads)(initial,initial,eval(a[1][1]))
            t_x = tf.keras.layers.LayerNormalization()(t_x+ initial)
            t_x1 = point_wise_feed_forward_network(d_model, dff)(t_x)
            globals()[a[1][-1]] = tf.keras.layers.LayerNormalization()(t_x+t_x1)
            outputs.append(eval(a[1][-1]))
            i = i+1
        else:
          t_x,_ = MultiHeadAttention(d_model, num_heads)(eval(a[1][0]),eval(a[1][0]),eval(a[1][1]))
          t_x1 = point_wise_feed_forward_network(d_model, dff)(t_x)
          globals()[a[1][-1]] = tf.keras.layers.LayerNormalization()(t_x+t_x1)
          outputs.append(eval(a[1][-1]))
          i = i+1
      else:
        pass
    final_output = outputs[-1]
    final_output = self.flatten(final_output)
    final_output = self.dropout(final_output)
    final_output = tf.keras.layers.Dense(3)(final_output)
    return final_output 

"""## Customised training"""




num_layers = 4
d_model = 30
maximum_position_encoding = 35
dff = 128
num_heads = 5
dropout_rate = 0.1
EPOCHS = 3
batch_size = 300

class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):
  def __init__(self, d_model, warmup_steps=4000):
    super(CustomSchedule, self).__init__()

    self.d_model = d_model
    self.d_model = tf.cast(self.d_model, tf.float32)

    self.warmup_steps = warmup_steps

  def __call__(self, step):
    arg1 = tf.math.rsqrt(step)
    arg2 = step * (self.warmup_steps ** -1.5)

    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)

learning_rate = CustomSchedule(d_model)



def loss_function(real, pred):
  loss_ = tf.keras.losses.categorical_crossentropy(real, pred)

  return loss_

def accuracy_function(real, pred):
  accuracies = tf.equal(tf.argmax(real,axis=1), tf.argmax(pred, axis=1))
  accuracies = tf.cast(accuracies, dtype=tf.float32)
  return accuracies

def F1Score(real, pred):
    real_ = tf.argmax(real,axis=1).numpy()
    pred_ = tf.argmax(pred, axis=1).numpy()
    aTP = 0
    aFP = 0
    aFN = 0
    bTP = 0
    bFP = 0
    bFN = 0
    cTP = 0
    cFP = 0
    cFN = 0
    for i in range(len(real_)):
        if pred_[i] == 0 and real_[i] == 0:
            aTP = aTP+1
        elif pred_[i] == 0 and real_[i] != 0:
            aFP= aFP+1
        elif pred_[i] != 0 and real_[i] == 0:
            aFN = aFN +1
        elif pred_[i] == 1 and real_[i] == 1:
            bTP = bTP+1
        elif pred_[i] == 1 and real_[i] != 1:
            bFP= bFP+1
        elif pred_[i] != 1 and real_[i] == 1:
            bFN = bFN +1
        elif pred_[i] == 2 and real_[i] == 2:
            cTP = cTP+1
        elif pred_[i] == 2 and real_[i] != 2:
            cFP= cFP+1
        elif pred_[i] != 2 and real_[i] == 2:
            cFN = cFN +1
    print(aTP,aFP,aFN)
    if aTP == 0:
        af1 = 0
    else:
        aprecision = aTP / (aTP + aFP)
        arecall = aTP / (aTP + aFN)
        af1 = 2 * aprecision * arecall / (aprecision + arecall)
    if bTP == 0:
        bf1 = 0
    else:
        bprecision = bTP / (bTP + bFP)
        brecall = bTP / (bTP + bFN)
        bf1 = 2 * bprecision * brecall / (bprecision + brecall)
    if cTP == 0:
        cf1 = 0
    else:
        cprecision = cTP / (cTP + cFP)
        crecall = cTP / (cTP + cFN)
        cf1 = 2 * cprecision * crecall / (cprecision + crecall)
    return (af1+bf1+cf1)/3

def train_ev (inp, tar, test_train, test_label, model):
  print(tf.config.list_physical_devices('GPU'))
  optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,epsilon=1e-9)
  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')
  train_loss = tf.keras.metrics.Mean(name='train_loss')
  train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')
  test_accuracy = tf.keras.metrics.Mean(name='test_accuracy')
  
  Model = build(model,dff, d_model, maximum_position_encoding, num_heads,batch_size)
  with tf.device('/gpu:0'):
    for epoch in range(EPOCHS):
      start = time.time()
      train_loss.reset_states()
      train_accuracy.reset_states()
      # inp -> indicators, tar -> predicted classification
      for batch, x in enumerate(range(0, len(inp), batch_size)):
        xs, ys = inp[x: x + batch_size], tar[x: x + batch_size]
        tar_inp = xs
        tar_real = ys
        with tf.GradientTape() as tape:
          predictions = Model(tar_inp, model, dff, d_model, num_heads,batch_size)
##          predictions = predictions[:, -1:, :]
##          predictions = tf.reshape(predictions[:, -1:, :],(predictions.shape[0], predictions.shape[2]))
          loss = loss_function(tar_real, predictions)
        gradients = tape.gradient(loss, Model.trainable_variables)
        optimizer.apply_gradients(zip(gradients, Model.trainable_variables))
        train_loss(loss)
        train_accuracy(accuracy_function(tar_real, predictions))
        if batch % 50 == 0:
          print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')
      print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')
      print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\n')
    Model.save_weights('GPU_trained.h5')
  with tf.device('/cpu:0'):
      Model.load_weights('GPU_trained.h5')
      tspredictions = Model(test_train, model, dff, d_model, num_heads,batch_size)
##      predictions = predictions[:, -1:, :]
##      predicted_id = tf.argmax(predictions, axis=-1)
##  print(count(predicted_id))
##      test_label_ = tf.argmax(test_label, axis=-1)
##      predictions = tf.reshape(predictions[:, -1:, :],(predictions.shape[0], predictions.shape[2]))
      test_accuracy(accuracy_function(test_label, tspredictions))
      test_acc = np.array(test_accuracy.result())
      Macro_f1= F1Score(test_label, tspredictions)
  print('test_acc: '+str(test_acc))
  print('Macro_f1: '+str(Macro_f1))
  return model, test_acc, Macro_f1


gpus = tf.config.list_physical_devices('GPU')
if gpus:
  try:
    # Currently, memory growth needs to be the same across GPUs
    for gpu in gpus:
      tf.config.experimental.set_memory_growth(gpu, True)
    logical_gpus = tf.config.experimental.list_logical_devices('GPU')
    print(len(gpus), "Physical GPUs,", len(logical_gpus), "Logical GPUs")
  except RuntimeError as e:
    # Memory growth must be set before GPUs have been initialized
    print(e)

"""## Searching method"""

def aging_evo (inp, tar, test_train, test_label, n_l):
  population = {}
  history = {}
  i = 1
  while len(population) < 25:
    model = randarchitecture (n_l)
    train_model, mod_acc, Macro_f1 = train_ev(inp, tar, test_train, test_label,model)
    population[i] = (train_model, mod_acc, Macro_f1)
    history[i] = (train_model, mod_acc, Macro_f1)
    i = i+1
  while len(history)<100:
    sample = {}
    while len(sample)<12:
      lst = list(population.keys())
      for _ in range (12):
        s = random.choice(lst)
        sample[s] = population[s]
    parent = max(sample.items(), key=lambda x: x[1][2])[0]
    parent_ = population[parent]
    parent_model = parent_[0]
    child = mutate(parent_model)
    print(child)
    child_model, child_acc, child_Macro_f1 = train_ev(inp, tar, test_train, test_label, child)
    print(child_acc,child_Macro_f1)
    population[i] = (child_model, child_acc, child_Macro_f1)
    history[i] = (child_model, child_acc, child_Macro_f1)
    i = i+1
    key = min (population.keys())
    print(len(history))
    del population[key]
    print(len(population))
  print('done3')
  final = history[max(history.items(), key=lambda x: x[1][2])[0]]
  return final, history

f_model, hst = aging_evo (X_train, y_train, X_test, y_test, 5)

print(f_model)
a_file = open("model.pkl", "wb")
pickle.dump(f_model, a_file)
a_file.close()

h_file = open("history.pkl", "wb")
pickle.dump(hst, h_file)
h_file.close()

with open('model.pkl', 'rb') as handle:
    model = pickle.load(handle)
model= model[0]

def finetuning(inp, tar, test_train, test_label, model):
    train_model, mod_acc, Macro_f1, (tspredictions) = train_ev(inp, tar, test_train, test_label,model)
    return train_model, mod_acc, Macro_f1, (tspredictions)

model, mod_acc, Macro_f1, tspredictions = finetuning(X_train, y_train, X_test, y_test, model)

final = (model, mod_acc, Macro_f1, tspredictions)

f_file = open("finalResult.pkl", "wb")
pickle.dump(final, f_file)
f_file.close()
